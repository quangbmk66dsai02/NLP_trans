{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "101\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from data.idiom_data.idiom_translation import TranslationDataset\n",
    "\n",
    "with open('data/idiom_data/total_idioms.txt') as f:\n",
    "    idioms = f.read()\n",
    "    idiomatic_sentences = idioms.split(\"\\n\")\n",
    "    \n",
    "with open('data/idiom_data/total_translated_idioms.txt') as f:\n",
    "    translated = f.read()\n",
    "    plain_sentences = translated.split(\"\\n\")\n",
    "\n",
    "\n",
    "print(len(idiomatic_sentences))\n",
    "print(len(plain_sentences))\n",
    "idiomatic_sentences = idiomatic_sentences[0:-1]\n",
    "plain_sentences = plain_sentences[0:-1]\n",
    "print(len(idiomatic_sentences))\n",
    "print(len(plain_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab and Tokenizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Helper function to tokenize and build vocabulary\n",
    "def yield_tokens(data_iter, tokenizer):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "vocab = build_vocab_from_iterator(yield_tokens(idiomatic_sentences + plain_sentences, tokenizer), specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "dataset = TranslationDataset(idiomatic_sentences, plain_sentences, vocab, tokenizer)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.idiom_model import Seq2Seq, Encoder, Decoder, Attention\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters\n",
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = len(vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "CLIP = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Init attention mask, encoder, and decoder\n",
    "attn = Attention(HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "# Init model\n",
    "idiom_model = Seq2Seq(enc, dec, device, vocab, tokenizer).to(device)\n",
    "\n",
    "# Define optimizer and Criterion\n",
    "optimizer = optim.Adam(idiom_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 5.470\n",
      "Epoch: 11, Train Loss: 0.764\n",
      "Epoch: 21, Train Loss: 0.066\n",
      "Epoch: 31, Train Loss: 0.013\n",
      "Epoch: 41, Train Loss: 0.005\n",
      "Epoch: 51, Train Loss: 0.002\n",
      "Epoch: 61, Train Loss: 0.022\n",
      "Epoch: 71, Train Loss: 0.004\n",
      "Epoch: 81, Train Loss: 0.002\n",
      "Epoch: 91, Train Loss: 0.001\n",
      "Epoch: 101, Train Loss: 0.001\n",
      "Epoch: 111, Train Loss: 0.001\n",
      "Epoch: 121, Train Loss: 0.001\n",
      "Epoch: 131, Train Loss: 0.001\n",
      "Epoch: 141, Train Loss: 0.001\n",
      "Epoch: 151, Train Loss: 0.000\n",
      "Epoch: 161, Train Loss: 0.216\n",
      "Epoch: 171, Train Loss: 0.009\n",
      "Epoch: 181, Train Loss: 0.057\n",
      "Epoch: 191, Train Loss: 0.013\n",
      "Epoch: 201, Train Loss: 0.001\n",
      "Epoch: 211, Train Loss: 0.000\n",
      "Epoch: 221, Train Loss: 0.000\n",
      "Epoch: 231, Train Loss: 0.001\n",
      "Epoch: 241, Train Loss: 0.000\n",
      "Epoch: 251, Train Loss: 0.000\n",
      "Epoch: 261, Train Loss: 0.000\n",
      "Epoch: 271, Train Loss: 0.000\n",
      "Epoch: 281, Train Loss: 0.000\n",
      "Epoch: 291, Train Loss: 0.000\n",
      "Epoch: 301, Train Loss: 0.000\n",
      "current loss 9.443262315471657e-05\n",
      "current loss 0.00013130527622706722\n",
      "current loss 0.0001301126270845998\n",
      "current loss 7.950549661472905e-05\n",
      "current loss 8.185971055354457e-05\n",
      "current loss 9.404161573911551e-05\n",
      "current loss 0.00010684098742785864\n",
      "current loss 7.43661388696637e-05\n",
      "current loss 9.10374139493797e-05\n",
      "current loss 7.716685249761212e-05\n",
      "Epoch: 311, Train Loss: 0.000\n",
      "current loss 0.0001238793116499437\n",
      "current loss 6.179473784868605e-05\n",
      "current loss 7.670579070691019e-05\n",
      "current loss 7.025411505310331e-05\n",
      "current loss 0.0001016038437228417\n",
      "current loss 0.00010823985794559122\n",
      "current loss 0.00012010836908302736\n",
      "current loss 8.938996797951404e-05\n",
      "current loss 0.006916790426112129\n",
      "current loss 0.0017509517092548776\n",
      "Epoch: 321, Train Loss: 0.000\n",
      "current loss 0.0004449990487046307\n",
      "current loss 0.0011883097198733595\n",
      "current loss 0.00020492177391133736\n",
      "current loss 0.0003958812689234037\n",
      "current loss 0.00040490235478500835\n",
      "current loss 0.00019487475183268543\n",
      "current loss 0.00029502281613531524\n",
      "current loss 0.00010962503984046634\n",
      "current loss 0.0019262310408521444\n",
      "current loss 0.027499692754645366\n",
      "Epoch: 331, Train Loss: 0.005\n",
      "current loss 0.0048809063766384496\n",
      "current loss 0.0006378112098900601\n",
      "current loss 0.017159833457844796\n",
      "current loss 0.07535101757166558\n",
      "current loss 0.04664414056460373\n",
      "current loss 0.046445357057382355\n",
      "current loss 0.04747042717935983\n",
      "current loss 0.06856741368537769\n",
      "current loss 0.06837974222726188\n",
      "current loss 0.1295355073583778\n",
      "Epoch: 341, Train Loss: 0.238\n",
      "current loss 0.23767831148579716\n",
      "current loss 0.08550047096796334\n",
      "current loss 0.024606261029839517\n",
      "current loss 0.07248056515818461\n",
      "current loss 0.05520766300614923\n",
      "current loss 0.10771565451286733\n",
      "current loss 0.013087895489297808\n",
      "current loss 0.02306695224833675\n",
      "current loss 0.006453107669949531\n",
      "current loss 0.019615353201515973\n",
      "Epoch: 351, Train Loss: 0.010\n",
      "current loss 0.01013474958599545\n",
      "current loss 0.03363495770609006\n",
      "current loss 0.002993846499884967\n",
      "current loss 0.009496348828542977\n",
      "current loss 0.0029848880250938235\n",
      "current loss 0.006027693409123458\n",
      "current loss 0.0575637262314558\n",
      "current loss 0.0027808885439299046\n",
      "current loss 0.022621341681224295\n",
      "current loss 0.0022980127483606338\n",
      "Epoch: 361, Train Loss: 0.002\n",
      "current loss 0.002109000366181135\n",
      "current loss 0.0012098555860575288\n",
      "current loss 0.00238093713123817\n",
      "current loss 0.0008793586210231296\n",
      "current loss 0.04481661829049699\n",
      "current loss 0.0006965083972318098\n",
      "current loss 0.007994372826942708\n",
      "current loss 0.015408231572655496\n",
      "current loss 0.002002277903375216\n",
      "current loss 0.0008828693804389332\n",
      "Epoch: 371, Train Loss: 0.003\n",
      "current loss 0.002718932279094588\n",
      "current loss 0.0011134712098282763\n",
      "current loss 0.009072198704234324\n",
      "current loss 0.004795769104384817\n",
      "current loss 0.0015894260963250418\n",
      "current loss 0.0028136351946159268\n",
      "current loss 0.002007362899166765\n",
      "current loss 0.009461939043103485\n",
      "current loss 0.021147500092774864\n",
      "current loss 0.008722454935195856\n",
      "Epoch: 381, Train Loss: 0.023\n",
      "current loss 0.023425454820971935\n",
      "current loss 0.005567034258274361\n",
      "current loss 0.009362197092559654\n",
      "current loss 0.0009162310059764422\n",
      "current loss 0.016700762449181637\n",
      "current loss 0.07500008084025467\n",
      "current loss 0.025218818579014625\n",
      "current loss 0.0267449507911806\n",
      "current loss 0.008911801908107008\n",
      "current loss 0.017890929462737404\n",
      "Epoch: 391, Train Loss: 0.001\n",
      "current loss 0.0008748708118218928\n",
      "current loss 0.0007520688028307632\n",
      "current loss 0.0009897157200612129\n",
      "current loss 0.000247304372896906\n",
      "current loss 0.0008768884603341575\n",
      "current loss 0.000422138469730271\n",
      "current loss 0.00041691157093737274\n",
      "current loss 0.0005607091210549697\n",
      "current loss 0.00030404629214899617\n",
      "current loss 0.00031102101020223926\n",
      "Epoch: 401, Train Loss: 0.000\n",
      "current loss 0.0002894447083235718\n",
      "current loss 0.00026011584159277843\n",
      "current loss 0.0002497034402040299\n",
      "current loss 0.0005699390196241439\n",
      "current loss 0.00026015672665380407\n",
      "current loss 0.0011254523473326118\n",
      "current loss 0.0003535103111062199\n",
      "current loss 0.0011666806050925516\n",
      "current loss 0.0038793966712546537\n",
      "current loss 0.00016980535583570598\n",
      "Epoch: 411, Train Loss: 0.000\n",
      "current loss 0.0002718055928198737\n",
      "current loss 0.0015788761993462685\n",
      "current loss 0.00015280105726560577\n",
      "current loss 0.0006657863988948521\n",
      "current loss 0.0003549714470864274\n",
      "current loss 0.00022133754991955358\n",
      "current loss 0.00034237789805047214\n",
      "current loss 0.00017979179210669827\n",
      "current loss 0.00023920567982713692\n",
      "current loss 0.00035457702142593917\n",
      "Epoch: 421, Train Loss: 0.002\n",
      "current loss 0.0015820955784874968\n",
      "current loss 0.0004141729325056076\n",
      "current loss 0.013528418442001566\n",
      "current loss 0.016279181604113548\n",
      "current loss 0.00037652612809324637\n",
      "current loss 0.00766581535935984\n",
      "current loss 0.0010472767800820293\n",
      "current loss 0.0006820992661232594\n",
      "current loss 0.0005827894550748169\n",
      "current loss 0.00035284809819131625\n",
      "Epoch: 431, Train Loss: 0.000\n",
      "current loss 0.000160503554798197\n",
      "current loss 0.0052157057834847365\n",
      "current loss 0.008961447946057887\n",
      "current loss 0.00016853528286446816\n",
      "current loss 0.014391135639743879\n",
      "current loss 0.01349359802188701\n",
      "current loss 0.006175966604496352\n",
      "current loss 0.05334233379762736\n",
      "current loss 0.0007956894769449718\n",
      "current loss 0.0007964967189764138\n",
      "Epoch: 441, Train Loss: 0.001\n",
      "current loss 0.0006899761880049482\n",
      "current loss 0.0005002747660910245\n",
      "current loss 0.0034208641707664357\n",
      "current loss 0.0013791004672384587\n",
      "current loss 0.01424146284989547\n",
      "current loss 0.0004544831928797066\n",
      "current loss 0.00047926744664437135\n",
      "current loss 0.0004466721868084278\n",
      "current loss 0.016415719379801887\n",
      "current loss 0.051496717514964985\n",
      "Epoch: 451, Train Loss: 0.054\n",
      "current loss 0.053846159933891614\n",
      "current loss 0.00018987224248121493\n",
      "current loss 0.03986681132519152\n",
      "current loss 0.012810183622059412\n",
      "current loss 0.001850694415406906\n",
      "current loss 0.000726928016229067\n",
      "current loss 0.0013084952403005445\n",
      "current loss 0.0015504311450058594\n",
      "current loss 0.00036999678632128053\n",
      "current loss 0.000783115118247224\n",
      "Epoch: 461, Train Loss: 0.001\n",
      "current loss 0.0010602313861454605\n",
      "current loss 0.0006118030178186018\n",
      "current loss 0.0003751418094907422\n",
      "current loss 0.00033121413034677973\n",
      "current loss 0.00020639897520595695\n",
      "current loss 0.0001961118065082701\n",
      "current loss 0.00013622440965264105\n",
      "current loss 0.00029727961773460263\n",
      "current loss 0.010020744296343764\n",
      "current loss 0.0020258854659914506\n",
      "Epoch: 471, Train Loss: 0.000\n",
      "current loss 0.00027323205540596973\n",
      "current loss 0.0007504100449295947\n",
      "current loss 0.00023939581515151077\n",
      "current loss 0.001868634543279768\n",
      "current loss 0.00034611485825735143\n",
      "current loss 0.00562064669575193\n",
      "current loss 0.015704046143946472\n",
      "current loss 0.002879443391793757\n",
      "current loss 0.0009400105620443356\n",
      "current loss 0.001453821574250469\n",
      "Epoch: 481, Train Loss: 0.003\n",
      "current loss 0.002860809743287973\n",
      "current loss 0.0004787796686287038\n",
      "current loss 0.0016754424152168213\n",
      "current loss 0.051021564343682255\n",
      "current loss 0.012381364793691318\n",
      "current loss 0.00255083922893391\n",
      "current loss 0.0005404692234151298\n",
      "current loss 0.0006567660804648768\n",
      "current loss 0.0013941587516455912\n",
      "current loss 0.0005944733500655274\n",
      "Epoch: 491, Train Loss: 0.001\n",
      "current loss 0.0012448837071133312\n",
      "current loss 0.00048269214858009946\n",
      "current loss 0.005118769011824042\n",
      "current loss 0.0011453488055849447\n",
      "current loss 0.003922903317834426\n",
      "current loss 0.001906297764799092\n",
      "current loss 0.0013552552183682565\n",
      "current loss 0.0002348012723814463\n",
      "current loss 0.0007039326992526185\n",
      "current loss 0.0007117961293261032\n",
      "Epoch: 501, Train Loss: 0.010\n",
      "current loss 0.010318385552818655\n",
      "current loss 0.010014196156407706\n",
      "current loss 0.0006309715958195738\n",
      "current loss 0.0005672390729159815\n",
      "current loss 0.0010282462699251482\n",
      "current loss 0.0007304045098862844\n",
      "current loss 0.0018470670038368553\n",
      "current loss 0.0024287518561322942\n",
      "current loss 0.0002677585176570574\n",
      "current loss 0.001046245616453234\n",
      "Epoch: 511, Train Loss: 0.000\n",
      "current loss 0.0002568017845987924\n",
      "current loss 0.0002961497888463782\n",
      "current loss 0.00012836112455261173\n",
      "current loss 0.00019396000170672777\n",
      "current loss 0.00017933260605786927\n",
      "current loss 0.00030241307158576094\n",
      "current loss 0.00021016260870965198\n",
      "current loss 7.015728851911263e-05\n",
      "current loss 0.0001856825109825877\n",
      "current loss 0.00011462713773653377\n",
      "Epoch: 521, Train Loss: 0.000\n",
      "current loss 0.00019295396905363304\n",
      "current loss 9.362124055769527e-05\n",
      "current loss 6.21385546764941e-05\n",
      "current loss 6.512147001558333e-05\n",
      "current loss 0.00013931266475992744\n",
      "current loss 0.00016181277987925569\n",
      "current loss 0.00011237047165195691\n",
      "current loss 6.074687971704407e-05\n",
      "current loss 6.345786769088591e-05\n",
      "current loss 7.194168738351436e-05\n",
      "Epoch: 531, Train Loss: 0.000\n",
      "current loss 6.25803575530881e-05\n",
      "current loss 6.760078904335387e-05\n",
      "current loss 6.379002443281933e-05\n",
      "current loss 0.0002340833641937934\n",
      "current loss 8.211442000174429e-05\n",
      "current loss 5.318548292052583e-05\n",
      "current loss 0.0006781183221391985\n",
      "current loss 0.00013039604491495992\n",
      "current loss 0.0002116135799951735\n",
      "current loss 7.811888026481029e-05\n",
      "Epoch: 541, Train Loss: 0.000\n",
      "current loss 5.2771230184589515e-05\n",
      "current loss 0.00019946593729400775\n",
      "current loss 0.0006998699052928714\n",
      "current loss 0.014999461049592355\n",
      "current loss 0.00014832633187324972\n",
      "current loss 0.0006030689579347382\n",
      "current loss 0.0009942794167727698\n",
      "current loss 0.0001340259284916101\n",
      "current loss 9.839052390816505e-05\n",
      "current loss 0.0006124529600128881\n",
      "Epoch: 551, Train Loss: 0.000\n",
      "current loss 0.0003408142812077131\n",
      "current loss 0.0001770359262081911\n",
      "current loss 0.001534950478344399\n",
      "current loss 8.182787078112597e-05\n",
      "current loss 7.618338750035037e-05\n",
      "current loss 9.30885123580083e-05\n",
      "current loss 0.0001025325804221211\n",
      "current loss 0.00014053949844310408\n",
      "current loss 0.0002068472180326353\n",
      "current loss 0.0004074052560099517\n",
      "Epoch: 561, Train Loss: 0.015\n",
      "current loss 0.014542018303927762\n",
      "current loss 0.001477261200261637\n",
      "current loss 6.865560389996972e-05\n",
      "current loss 0.0001302182226027071\n",
      "current loss 0.00015436562262038933\n",
      "current loss 0.0003173504184815101\n",
      "current loss 8.530631403118605e-05\n",
      "current loss 3.804373100138037e-05\n",
      "current loss 0.0004961906390235527\n",
      "current loss 0.00022499999813589965\n",
      "Epoch: 571, Train Loss: 0.000\n",
      "current loss 0.0001728183181512577\n",
      "current loss 0.0018085999521645136\n",
      "current loss 0.005411295493831858\n",
      "current loss 0.001394879789040715\n",
      "current loss 0.0010398937793070218\n",
      "current loss 0.012816300491249422\n",
      "current loss 0.0002138300256774528\n",
      "current loss 0.010710807071700402\n",
      "current loss 0.00819688470874098\n",
      "current loss 0.00014413319186132867\n",
      "Epoch: 581, Train Loss: 0.011\n",
      "current loss 0.010557793123189186\n",
      "current loss 0.0008847415883792564\n",
      "current loss 0.014719189763854956\n",
      "current loss 0.021270364267911644\n",
      "current loss 0.006652407352885348\n",
      "current loss 0.0373711892441861\n",
      "current loss 0.034186588137526994\n",
      "current loss 0.0013196303872973659\n",
      "current loss 0.0031098435167223213\n",
      "current loss 0.0007063775185088162\n",
      "Epoch: 591, Train Loss: 0.002\n",
      "current loss 0.0022811906499555336\n",
      "current loss 0.010975731510552578\n",
      "current loss 0.005053379082892206\n",
      "current loss 0.0019253010526881554\n",
      "current loss 0.0015474668089154875\n",
      "current loss 0.0006907860572027858\n",
      "current loss 0.0012895246320113074\n",
      "current loss 0.00025454407950746826\n",
      "current loss 0.0016553607036257744\n",
      "current loss 0.0006948502374143573\n",
      "Epoch: 601, Train Loss: 0.000\n",
      "current loss 0.0004289169934054371\n",
      "current loss 0.0002393453081822372\n",
      "current loss 0.00027672255773723007\n",
      "current loss 0.0014113807254943823\n",
      "current loss 0.0001925838216266129\n",
      "current loss 0.00033992373246292116\n",
      "current loss 0.0003189554823620711\n",
      "current loss 0.019555270672935877\n",
      "current loss 0.0007087111731379992\n",
      "current loss 0.046063337118175694\n",
      "Epoch: 611, Train Loss: 0.000\n",
      "current loss 0.00041957057765102946\n",
      "current loss 9.735428720887284e-05\n",
      "current loss 0.000998128251376329\n",
      "current loss 0.0002269102209538687\n",
      "current loss 0.0006233084121049614\n",
      "current loss 0.00011088919018220622\n",
      "current loss 0.00023448820702469674\n",
      "current loss 0.0003665120583718817\n",
      "current loss 0.00029318154374777803\n",
      "current loss 0.0027866038102729363\n",
      "Epoch: 621, Train Loss: 0.000\n",
      "current loss 0.00013712619638681645\n",
      "current loss 0.01259864047406154\n",
      "current loss 0.05178115104272365\n",
      "current loss 0.0019211157896279474\n",
      "current loss 0.0013819528605381493\n",
      "current loss 0.0547377549995872\n",
      "current loss 0.04851668696283014\n",
      "current loss 0.017597680483959266\n",
      "current loss 0.02482928153622197\n",
      "current loss 0.022728464863757836\n",
      "Epoch: 631, Train Loss: 0.069\n",
      "current loss 0.0685621049895417\n",
      "current loss 0.014557468585553578\n",
      "current loss 0.01567987829876074\n",
      "current loss 0.0026008536464360078\n",
      "current loss 0.012215439397550654\n",
      "current loss 0.04146622462067171\n",
      "current loss 0.0010809697807417252\n",
      "current loss 0.004760719946352765\n",
      "current loss 0.0041711759338795675\n",
      "current loss 0.016368320823676185\n",
      "Epoch: 641, Train Loss: 0.021\n",
      "current loss 0.021374355620355345\n",
      "current loss 0.005045573879760923\n",
      "current loss 0.0005744610858528177\n",
      "current loss 0.0016566102611250244\n",
      "current loss 0.0014807503874180838\n",
      "current loss 0.0013897270229790593\n",
      "current loss 0.0003808074281550944\n",
      "current loss 0.0003185408419085434\n",
      "current loss 0.001715188032540027\n",
      "current loss 0.0004851874913583742\n",
      "Epoch: 651, Train Loss: 0.002\n",
      "current loss 0.0016440696323115843\n",
      "current loss 0.00047849012771621345\n",
      "current loss 0.007407978859191644\n",
      "current loss 0.0013032704573561205\n",
      "current loss 0.00039938693917065395\n",
      "current loss 0.001257856706797611\n",
      "current loss 0.010878075133223319\n",
      "current loss 0.00017014022560033482\n",
      "current loss 0.0001305226433032658\n",
      "current loss 0.0004924463188217487\n",
      "Epoch: 661, Train Loss: 0.001\n",
      "current loss 0.0005751104364208004\n",
      "current loss 0.000796549413826142\n",
      "current loss 0.00032304892783940886\n",
      "current loss 0.0163568064048377\n",
      "current loss 0.00023675643642491196\n",
      "current loss 0.00026093626438523644\n",
      "current loss 0.03585785037048481\n",
      "current loss 0.00795598808927025\n",
      "current loss 0.0002585428817837965\n",
      "current loss 0.050070108659565446\n",
      "Epoch: 671, Train Loss: 0.001\n",
      "current loss 0.000896993204150931\n",
      "current loss 0.009775419564903132\n",
      "current loss 0.0011263507222793122\n",
      "current loss 0.02055921405990375\n",
      "current loss 0.005851729743335454\n",
      "current loss 0.00038567526789847764\n",
      "current loss 0.0003624290431616828\n",
      "current loss 0.00014240468772186432\n",
      "current loss 0.00034985656866410865\n",
      "current loss 0.0001726297496134066\n",
      "Epoch: 681, Train Loss: 0.001\n",
      "current loss 0.0005261043515929487\n",
      "current loss 0.0001599177638127003\n",
      "current loss 0.00021194928885961417\n",
      "current loss 5.7883649878931466e-05\n",
      "current loss 0.00010905209028351237\n",
      "current loss 0.000334389484305575\n",
      "current loss 0.00026955235844070555\n",
      "current loss 0.0001201746041260776\n",
      "current loss 8.335313741554273e-05\n",
      "current loss 7.808563932485413e-05\n",
      "Epoch: 691, Train Loss: 0.000\n",
      "current loss 0.00012525738111435203\n",
      "current loss 6.70593251925311e-05\n",
      "current loss 8.076496187641169e-05\n",
      "current loss 5.3900506827631034e-05\n",
      "current loss 0.00024997752443596256\n",
      "current loss 4.9977686649071985e-05\n",
      "current loss 4.787967704942275e-05\n",
      "current loss 5.969345529592829e-05\n",
      "current loss 0.00029871298170291993\n",
      "current loss 6.635300524067133e-05\n",
      "Epoch: 701, Train Loss: 0.000\n",
      "current loss 0.00037382790324045343\n",
      "current loss 0.00029259161919981125\n",
      "current loss 0.0011164452989305574\n",
      "current loss 0.0006929931058948568\n",
      "current loss 0.06503851763573039\n",
      "current loss 0.00031801956865820105\n",
      "current loss 0.0070297803536959694\n",
      "current loss 0.0006500729148683603\n",
      "current loss 7.482344881282188e-05\n",
      "current loss 0.00019818001328530954\n",
      "Epoch: 711, Train Loss: 0.001\n",
      "current loss 0.0009624866709714297\n",
      "current loss 0.026990385881708788\n",
      "current loss 0.003958581842016429\n",
      "current loss 0.0033826913713710383\n",
      "current loss 0.0027551719947950916\n",
      "current loss 0.0003973214580128115\n",
      "current loss 0.00043446083818707846\n",
      "current loss 7.871912539485493e-05\n",
      "current loss 0.0002934688551249565\n",
      "current loss 0.00012778943510056707\n",
      "Epoch: 721, Train Loss: 0.000\n",
      "current loss 8.032240862121398e-05\n",
      "current loss 0.00012462446557037765\n",
      "current loss 8.817131019895896e-05\n",
      "current loss 0.0001235727451785351\n",
      "current loss 0.00018847616597668094\n",
      "current loss 0.0001435604280231928\n",
      "current loss 0.00010562832330833772\n",
      "current loss 4.883408855675953e-05\n",
      "current loss 4.461076441657497e-05\n",
      "current loss 4.2032840974570716e-05\n",
      "Epoch: 731, Train Loss: 0.000\n",
      "current loss 4.331190739321755e-05\n",
      "current loss 8.821949186312849e-05\n",
      "current loss 0.00015924172366794665\n",
      "current loss 6.590441871594521e-05\n",
      "current loss 3.376237846168806e-05\n",
      "current loss 5.8115211868425834e-05\n",
      "current loss 0.0003185641894560831\n",
      "current loss 0.00027772300181823085\n",
      "current loss 0.00010817122872595065\n",
      "current loss 4.548371107375715e-05\n",
      "Epoch: 741, Train Loss: 0.000\n",
      "current loss 0.00010251041148876539\n",
      "current loss 0.00015710800707893213\n",
      "current loss 0.0002336013189960795\n",
      "current loss 1.6155825505848043e-05\n",
      "current loss 0.0001234878737705003\n",
      "current loss 2.6650800828065256e-05\n",
      "current loss 4.683462152570428e-05\n",
      "current loss 5.464034234137216e-05\n",
      "current loss 2.937087288046314e-05\n",
      "current loss 2.1180379053475918e-05\n",
      "Epoch: 751, Train Loss: 0.000\n",
      "current loss 8.129066709443578e-05\n",
      "current loss 8.986286072740768e-05\n",
      "current loss 1.3545496835831728e-05\n",
      "current loss 3.6082956603422646e-05\n",
      "current loss 0.00013093346424284392\n",
      "current loss 2.2492757670988796e-05\n",
      "current loss 2.644652931849123e-05\n",
      "current loss 0.00015324929236157914\n",
      "current loss 4.9630752437224145e-05\n",
      "current loss 4.5861089074605846e-05\n",
      "Epoch: 761, Train Loss: 0.000\n",
      "current loss 4.045275632051926e-05\n",
      "current loss 0.0006310049513558625\n",
      "current loss 2.6750972028821707e-05\n",
      "current loss 0.0001905603369777964\n",
      "current loss 0.00011268098278378602\n",
      "current loss 4.112760825591977e-05\n",
      "current loss 0.00025090112403631793\n",
      "current loss 0.00012295334481677856\n",
      "current loss 9.425935584204126e-05\n",
      "current loss 1.4419974513657508e-05\n",
      "Epoch: 771, Train Loss: 0.000\n",
      "current loss 1.4544200575983269e-05\n",
      "current loss 2.7910075914405752e-05\n",
      "current loss 2.5115372591244522e-05\n",
      "current loss 1.7720923005981602e-05\n",
      "current loss 5.442339370347327e-05\n",
      "current loss 3.4867091835621976e-05\n",
      "current loss 0.00010860479251277866\n",
      "current loss 4.08249128213356e-05\n",
      "current loss 2.5172649566229664e-05\n",
      "current loss 3.371575573964947e-05\n",
      "Epoch: 781, Train Loss: 0.000\n",
      "current loss 2.1049043562015866e-05\n",
      "current loss 5.801237525702163e-05\n",
      "current loss 2.7695690005202777e-05\n",
      "current loss 1.5021593117126032e-05\n",
      "current loss 1.8982558617608448e-05\n",
      "current loss 1.9790403621300355e-05\n",
      "current loss 4.40979647464701e-05\n",
      "current loss 9.432206138626498e-05\n",
      "current loss 7.085793306487176e-05\n",
      "current loss 2.6058053072119948e-05\n",
      "Epoch: 791, Train Loss: 0.000\n",
      "current loss 1.201523533609361e-05\n",
      "current loss 4.11179807656481e-05\n",
      "current loss 1.5969284913808223e-05\n",
      "current loss 1.228378228006477e-05\n",
      "current loss 1.7937663415068528e-05\n",
      "current loss 4.3492433064784564e-05\n",
      "current loss 2.129646209141356e-05\n",
      "current loss 3.697607858157426e-05\n",
      "current loss 2.9469693572536926e-05\n",
      "current loss 2.3314577174460282e-05\n",
      "Epoch: 801, Train Loss: 0.000\n",
      "current loss 8.030552739910491e-05\n",
      "current loss 0.0007274582734908108\n",
      "current loss 0.0027487922495765815\n",
      "current loss 2.341861036256887e-05\n",
      "current loss 2.182029729738133e-05\n",
      "current loss 1.3470156636685715e-05\n",
      "current loss 3.9146683957369535e-05\n",
      "current loss 9.393464756612957e-05\n",
      "current loss 1.1978906945842027e-05\n",
      "current loss 1.8569538815427224e-05\n",
      "Epoch: 811, Train Loss: 0.000\n",
      "current loss 2.3263299135578562e-05\n",
      "current loss 2.0295804097258953e-05\n",
      "current loss 2.6843895034289746e-05\n",
      "current loss 3.330963168082235e-05\n",
      "current loss 2.202086752731702e-05\n",
      "current loss 9.105830622502253e-05\n",
      "current loss 4.3590957238848206e-05\n",
      "current loss 4.5075762182023026e-05\n",
      "current loss 8.018191304017818e-05\n",
      "current loss 3.912146894435864e-05\n",
      "Epoch: 821, Train Loss: 0.000\n",
      "current loss 3.609042591961043e-05\n",
      "current loss 0.0001374573285374936\n",
      "current loss 5.880505782442924e-05\n",
      "current loss 5.316496151408501e-05\n",
      "current loss 3.0370681406566292e-05\n",
      "current loss 3.442012960022112e-05\n",
      "current loss 2.7336811342593137e-05\n",
      "current loss 1.2756871319652418e-05\n",
      "current loss 1.1577008251606458e-05\n",
      "current loss 2.169531151139381e-05\n",
      "Epoch: 831, Train Loss: 0.000\n",
      "current loss 2.1462752397383155e-05\n",
      "current loss 6.668917512797634e-05\n",
      "current loss 1.807974194889539e-05\n",
      "current loss 4.4528876924232466e-05\n",
      "current loss 1.4746241538432514e-05\n",
      "current loss 2.0148205499026518e-05\n",
      "current loss 3.8031114900149984e-05\n",
      "current loss 0.00026358189834354564\n",
      "current loss 1.5195983792182233e-05\n",
      "current loss 0.00016318869820679537\n",
      "Epoch: 841, Train Loss: 0.000\n",
      "current loss 1.5059812244544447e-05\n",
      "current loss 2.3167345352703707e-05\n",
      "current loss 3.6234487515685033e-05\n",
      "current loss 2.716197161589662e-05\n",
      "current loss 3.622932348434915e-05\n",
      "current loss 9.294532719650306e-06\n",
      "current loss 2.6851847178477328e-05\n",
      "current loss 3.513319243211299e-05\n",
      "current loss 1.1859019105031621e-05\n",
      "current loss 1.205728783588711e-05\n",
      "Epoch: 851, Train Loss: 0.000\n",
      "current loss 1.2849232325606863e-05\n",
      "current loss 0.02228342350422281\n",
      "current loss 7.221571858053721e-05\n",
      "current loss 0.0025755086299795948\n",
      "current loss 8.593677723638393e-05\n",
      "current loss 0.00022861271115743876\n",
      "current loss 9.175522991426988e-05\n",
      "current loss 0.0012047807454564463\n",
      "current loss 0.0004931449690502631\n",
      "current loss 0.0008334674131219799\n",
      "Epoch: 861, Train Loss: 0.000\n",
      "current loss 5.75552650843747e-05\n",
      "current loss 3.4118124858650845e-05\n",
      "current loss 0.000947182434174465\n",
      "current loss 0.0001437288041870488\n",
      "current loss 0.0004274605875252746\n",
      "current loss 0.0019856039577689443\n",
      "current loss 0.0010789111556960052\n",
      "current loss 0.037265252662746204\n",
      "current loss 0.042427699441759614\n",
      "current loss 0.006095025754984817\n",
      "Epoch: 871, Train Loss: 0.005\n",
      "current loss 0.0052962596579163804\n",
      "current loss 0.005238598402502248\n",
      "current loss 0.0019695260015396344\n",
      "current loss 0.014866349704152525\n",
      "current loss 0.011846585514285834\n",
      "current loss 0.013568051032052608\n",
      "current loss 0.0015000065005551732\n",
      "current loss 0.03571431090385886\n",
      "current loss 0.0002755361965682823\n",
      "current loss 0.03668698779219994\n",
      "Epoch: 881, Train Loss: 0.116\n",
      "current loss 0.11557062465053605\n",
      "current loss 0.054858570469514235\n",
      "current loss 0.01039538461554912\n",
      "current loss 0.08466606508800396\n",
      "current loss 0.03951593647070695\n",
      "current loss 0.07275371348659973\n",
      "current loss 0.02955178111587884\n",
      "current loss 0.011852970701875165\n",
      "current loss 0.10706908649299293\n",
      "current loss 0.010024563452498114\n",
      "Epoch: 891, Train Loss: 0.096\n",
      "current loss 0.0964938138888101\n",
      "current loss 0.06933657893241615\n",
      "current loss 0.054814062733203175\n",
      "current loss 0.15042280131892766\n",
      "current loss 0.016672104742247028\n",
      "current loss 0.11726125567220151\n",
      "current loss 0.006916973226179834\n",
      "current loss 0.000951070662267739\n",
      "current loss 0.0017431553511414676\n",
      "current loss 0.021400224595709005\n",
      "Epoch: 901, Train Loss: 0.026\n",
      "current loss 0.026379622786771507\n",
      "current loss 0.000285744290886214\n",
      "current loss 0.007118437569079106\n",
      "current loss 0.002750782671137131\n",
      "current loss 0.0013657313293151674\n",
      "current loss 0.0004830400873288454\n",
      "current loss 0.00033772140959626996\n",
      "current loss 0.0012997135248951964\n",
      "current loss 7.807414858689299e-05\n",
      "current loss 0.005314195269420452\n",
      "Epoch: 911, Train Loss: 0.001\n",
      "current loss 0.0009411884486326017\n",
      "current loss 0.017648560212455776\n",
      "current loss 0.0011116287470940734\n",
      "current loss 0.0005684531648512348\n",
      "current loss 0.004837781960122811\n",
      "current loss 0.00196604002831009\n",
      "current loss 0.001450080922950292\n",
      "current loss 0.0019843384296109436\n",
      "current loss 0.001564031662655907\n",
      "current loss 0.0005246222372988996\n",
      "Epoch: 921, Train Loss: 0.000\n",
      "current loss 0.0004356866364105372\n",
      "current loss 0.001287981078348821\n",
      "current loss 0.0004248179753631121\n",
      "current loss 0.00031147257477641687\n",
      "current loss 0.0001122944091548561\n",
      "current loss 0.0006056071646980854\n",
      "current loss 0.00048800794381804736\n",
      "current loss 0.00045741700723738176\n",
      "current loss 0.000131764847174054\n",
      "current loss 0.00011639484939678369\n",
      "Epoch: 931, Train Loss: 0.003\n",
      "current loss 0.002618173876180663\n",
      "current loss 9.878866230792483e-05\n",
      "current loss 0.0006587468611542136\n",
      "current loss 0.00047400829316757154\n",
      "current loss 0.0012817203781651187\n",
      "current loss 0.027385550709641392\n",
      "current loss 9.525902805762598e-05\n",
      "current loss 0.0040293902407938734\n",
      "current loss 0.00024765618500168787\n",
      "current loss 0.00018322929145142552\n",
      "Epoch: 941, Train Loss: 0.003\n",
      "current loss 0.002727977106405888\n",
      "current loss 0.00029099205321472257\n",
      "current loss 0.0011427496522628644\n",
      "current loss 0.02676097318023949\n",
      "current loss 0.0023646495214052266\n",
      "current loss 0.002250731961703423\n",
      "current loss 0.009844488604903745\n",
      "current loss 0.0004329438029344601\n",
      "current loss 0.00033158445748995293\n",
      "current loss 7.158486969274235e-05\n",
      "Epoch: 951, Train Loss: 0.002\n",
      "current loss 0.0015230704233317738\n",
      "current loss 0.00010557033008353755\n",
      "current loss 0.02263763851594831\n",
      "current loss 0.0013530790735785559\n",
      "current loss 0.0013091155216898187\n",
      "current loss 0.00036020442967128475\n",
      "current loss 0.0002820859729581571\n",
      "current loss 0.054514790335383624\n",
      "current loss 0.000803817126143258\n",
      "current loss 0.00010623061743899597\n",
      "Epoch: 961, Train Loss: 0.001\n",
      "current loss 0.0007175940221372911\n",
      "current loss 0.0002081829120470502\n",
      "current loss 0.0004819266194772354\n",
      "current loss 0.00984735498336704\n",
      "current loss 0.001192754711064481\n",
      "current loss 0.012186530306644272\n",
      "current loss 9.302948942604416e-05\n",
      "current loss 0.00016210933563343133\n",
      "current loss 9.249169088434427e-05\n",
      "current loss 0.09355034504023933\n",
      "Epoch: 971, Train Loss: 0.000\n",
      "current loss 0.0004036842026835075\n",
      "current loss 0.0036939176217856582\n",
      "current loss 8.058801486185985e-05\n",
      "current loss 8.950398460001452e-05\n",
      "current loss 6.245220706659892e-05\n",
      "current loss 0.00013305701195349685\n",
      "current loss 0.00021678196978882624\n",
      "current loss 5.8321572578279304e-05\n",
      "current loss 0.00028854086149294745\n",
      "current loss 7.054113566482557e-05\n",
      "Epoch: 981, Train Loss: 0.000\n",
      "current loss 0.0001069086328470803\n",
      "current loss 5.4760153670940784e-05\n",
      "current loss 2.2148018524603684e-05\n",
      "current loss 2.763745335414569e-05\n",
      "current loss 6.660318431386258e-05\n",
      "current loss 0.0003630503806334673\n",
      "current loss 1.3901427382734256e-05\n",
      "current loss 2.5414731589989968e-05\n",
      "current loss 0.00032294315410581477\n",
      "current loss 0.0008480131351461751\n",
      "Epoch: 991, Train Loss: 0.000\n",
      "current loss 2.5124354658601077e-05\n",
      "current loss 1.424792476427683e-05\n",
      "current loss 0.00018715208782396075\n",
      "current loss 3.0798456657521454e-05\n",
      "current loss 1.8001643002207857e-05\n",
      "current loss 0.00029649236644218034\n",
      "current loss 4.366432572169288e-05\n",
      "current loss 2.3661491309212578e-05\n",
      "current loss 0.0003093460560194217\n",
      "current loss 2.1425835802801886e-05\n",
      "Epoch: 1001, Train Loss: 0.000\n",
      "current loss 2.7360676199350564e-05\n"
     ]
    }
   ],
   "source": [
    "# Train_loop (details train_model in models.idiom_model)\n",
    "losses = []\n",
    "for epoch in range(1001):\n",
    "    # print(\"start of epoch\", epoch, \"========================================\")\n",
    "    train_loss = idiom_model.train_model(dataloader=dataloader,optimizer=optimizer,criterion=criterion,clip=CLIP,vocab=vocab)\n",
    "    losses.append(train_loss)\n",
    "    if epoch %10 == 0:\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}')\n",
    "        pass\n",
    "    if epoch >= 300:\n",
    "        print(\"current loss\", train_loss)\n",
    "        tmp_chk = True\n",
    "        for i in range(5):\n",
    "            if losses[-i] >= 0.001:\n",
    "                tmp_chk = False\n",
    "        if tmp_chk == True:\n",
    "            print(\"Early Stopping at epoch\", epoch)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_path = ('models_checkpoint/idiom_model.pth')  \n",
    "torch.save(idiom_model.state_dict(), save_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Testing section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init a new model\n",
    "new_model = Seq2Seq(enc,dec,device,vocab,tokenizer)\n",
    "new_model.load_state_dict(torch.load(save_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: <unk> in poor condition\n"
     ]
    }
   ],
   "source": [
    "sentence = \"in a really bad shape\"\n",
    "generated_sentence = new_model.sample(sentence)\n",
    "print(\"Generated Sentence:\", generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Generated Sentence: <unk> as a precaution\n",
      "1 Generated Sentence: <unk> something pitiful or disappointing to see\n",
      "2 Generated Sentence: <unk> general guideline\n",
      "3 Generated Sentence: <unk> seize the weather\n",
      "4 Generated Sentence: <unk> youthful times\n",
      "5 Generated Sentence: <unk> unofficially\n",
      "6 Generated Sentence: <unk>\n",
      "7 Generated Sentence: <unk> lot of money\n",
      "8 Generated Sentence: <unk> hottest days of summer\n",
      "9 Generated Sentence: <unk> inexperienced\n",
      "10 Generated Sentence: <unk> deserved outcome\n",
      "11 Generated Sentence: <unk> expensive\n",
      "12 Generated Sentence: <unk> forget it\n",
      "13 Generated Sentence: <unk> physical buildings\n",
      "14 Generated Sentence: <unk> narrow escape\n",
      "15 Generated Sentence: <unk> a welcome sight\n",
      "16 Generated Sentence: <unk> obvious conflict\n",
      "17 Generated Sentence: <unk> small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of money small amount of\n",
      "18 Generated Sentence: <unk> success on the third attempt\n",
      "19 Generated Sentence: <unk> rushing to see\n",
      "20 Generated Sentence: <unk> no matter the weather\n",
      "21 Generated Sentence: <unk> wait a moment\n",
      "22 Generated Sentence: <unk> almost free\n",
      "23 Generated Sentence: <unk> close together\n",
      "24 Generated Sentence: <unk>\n",
      "25 Generated Sentence: <unk> useless people or things\n",
      "26 Generated Sentence: <unk> but possible\n",
      "27 Generated Sentence: <unk> very unlikely\n",
      "28 Generated Sentence: <unk> graceful movement\n",
      "29 Generated Sentence: <unk> very dressed up\n",
      "30 Generated Sentence: <unk> alternative method\n",
      "31 Generated Sentence: <unk> conditions\n",
      "32 Generated Sentence: <unk> a boost\n",
      "33 Generated Sentence: <unk> of surprise or admiration\n",
      "34 Generated Sentence: <unk> very kind generous\n",
      "35 Generated Sentence: <unk> being planned or developed\n",
      "36 Generated Sentence: <unk> period of great achievement\n",
      "37 Generated Sentence: <unk> a state of uncertainty\n",
      "38 Generated Sentence: <unk> difficult introduction\n",
      "39 Generated Sentence: <unk> indescribable\n",
      "40 Generated Sentence: <unk> available for discussion available for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for discussion for\n",
      "41 Generated Sentence: <unk> in poor condition\n",
      "42 Generated Sentence: <unk> facing facing a difficult choice\n",
      "43 Generated Sentence: <unk>\n",
      "44 Generated Sentence: <unk> correct\n",
      "45 Generated Sentence: <unk> high five\n",
      "46 Generated Sentence: <unk> overwhelmed by emotion\n",
      "47 Generated Sentence: <unk> healthy\n",
      "48 Generated Sentence: <unk> take care of your family first\n",
      "49 Generated Sentence: <unk> laxative\n",
      "50 Generated Sentence: <unk> hurry\n",
      "51 Generated Sentence: <unk> fitting into any specific category\n",
      "52 Generated Sentence: <unk> securely fastened or displayed\n",
      "53 Generated Sentence: <unk> briefly\n",
      "54 Generated Sentence: <unk> caught in the act\n",
      "55 Generated Sentence: <unk> nearby or admiration\n",
      "56 Generated Sentence: <unk> set procedure or routine\n",
      "57 Generated Sentence: <unk> savings\n",
      "58 Generated Sentence: <unk> duration of usability\n",
      "59 Generated Sentence: <unk> or discarded items miscellaneous or discarded items miscellaneous or discarded items\n",
      "60 Generated Sentence: <unk> enjoying nightlife enjoying nightlife\n",
      "61 Generated Sentence: <unk> undecided\n",
      "62 Generated Sentence: <unk> unattached and unattached unattached unattached unattached unattached unattached unattached unattached and unattached unattached unattached and unattached unattached and unattached unattached and unattached unattached and unattached unattached and unattached unattached and unattached unattached and unattached unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached and unattached\n",
      "63 Generated Sentence: <unk> in love\n",
      "64 Generated Sentence: <unk> immediately\n",
      "65 Generated Sentence: <unk> fitting retribution\n",
      "66 Generated Sentence: <unk> legally\n",
      "67 Generated Sentence: <unk> sound asleep\n",
      "68 Generated Sentence: <unk> items\n",
      "69 Generated Sentence: <unk> beautiful\n",
      "70 Generated Sentence: <unk> easy money\n",
      "71 Generated Sentence: <unk> routine work\n",
      "72 Generated Sentence: <unk> knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading knowledgeable from reading\n",
      "73 Generated Sentence: <unk> not young\n",
      "74 Generated Sentence: <unk> n't disturb a situation\n",
      "75 Generated Sentence: <unk> harder to do than to say\n",
      "76 Generated Sentence: <unk> expression of surprise or admiration\n",
      "77 Generated Sentence: <unk> wait and generous\n",
      "78 Generated Sentence: <unk> not\n",
      "79 Generated Sentence: <unk> affecting everyone equally\n",
      "80 Generated Sentence: <unk> in a hurry\n",
      "81 Generated Sentence: <unk> in the same difficult situation\n",
      "82 Generated Sentence: <unk> run - down area\n",
      "83 Generated Sentence: <unk> the advantages of two different things\n",
      "84 Generated Sentence: <unk> basic details\n",
      "85 Generated Sentence: <unk> powerful but anonymous people\n",
      "86 Generated Sentence: <unk> involved in a deadline\n",
      "87 Generated Sentence: <unk> at of survival\n",
      "88 Generated Sentence: <unk> is valuable\n",
      "89 Generated Sentence: <unk> young energetic people\n",
      "90 Generated Sentence: <unk> work done for passion , not money\n",
      "91 Generated Sentence: <unk> crucial test\n",
      "92 Generated Sentence: <unk> completely finished\n",
      "93 Generated Sentence: <unk> poor\n",
      "94 Generated Sentence: <unk> sought - after goal\n",
      "95 Generated Sentence: <unk> until now\n",
      "96 Generated Sentence: <unk> perfectly honest or pure\n",
      "97 Generated Sentence: <unk> everyone together\n",
      "98 Generated Sentence: <unk> crime committed by an insider\n",
      "99 Generated Sentence: <unk> special talent\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,100):\n",
    "    sentence = idiomatic_sentences[i]\n",
    "    generated_sentence = idiom_model.sample(sentence)\n",
    "    print(i, \"Generated Sentence:\", generated_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
