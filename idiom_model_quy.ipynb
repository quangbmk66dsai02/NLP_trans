{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quang/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/quang/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/quang/anaconda3/envs/pytorch3d/lib/python3.9/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import random\n",
    "\n",
    "# Define the encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "# Define the attention mechanism\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        hidden = hidden[-1].unsqueeze(0).repeat(src_len, 1, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        return nn.functional.softmax(attention, dim=0)\n",
    "\n",
    "# Define the decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # print(\"enter decoder forward\")\n",
    "        input = input.unsqueeze(0)  # input shape: [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(input))  # embedded shape: [1, batch_size, emb_dim]\n",
    "        # print(\"finished embeded\")\n",
    "        # print(\"hidden shape\", hidden.shape)\n",
    "        # print(\"encoder_output shape\", encoder_outputs.shape)\n",
    "\n",
    "        a = self.attention(hidden, encoder_outputs)  # a shape: [src_len, batch_size]\n",
    "        # print(\"finished attention\")\n",
    "        a = a.unsqueeze(1)  # a shape: [src_len, 1, batch_size]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)  # encoder_outputs shape: [batch_size, src_len, hid_dim]\n",
    "\n",
    "        weighted = torch.bmm(a.permute(2, 1, 0), encoder_outputs)  # weighted shape: [batch_size, 1, hid_dim]\n",
    "        weighted = weighted.permute(1, 0, 2)  # weighted shape: [1, batch_size, hid_dim]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)  # rnn_input shape: [1, batch_size, emb_dim + hid_dim]\n",
    "\n",
    "        output, hidden = self.rnn(rnn_input, hidden)  # output shape: [1, batch_size, hid_dim]\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))  # prediction shape: [batch_size, output_dim]\n",
    "        return prediction, hidden\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # print(\"enter forward\")\n",
    "        trg_len = trg.shape[0]\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        # print(\"done hidden\")\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_len):\n",
    "            # print(\"before decoder\")\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            # print(\"finished decoder\")\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
    "        return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, vocab, tokenizer):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.vocab = vocab  # Add vocab as a class attribute\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def forward(self, src, trg=None, teacher_forcing_ratio=0.5, mode='train'):\n",
    "        trg_len = trg.shape[0] if trg is not None else 100  # Set a maximum length for inference\n",
    "        batch_size = src.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        input = trg[0, :] if trg is not None else torch.zeros(batch_size, dtype=torch.long).to(self.device)  # Start tokens or zero for inference\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            if mode == 'train':\n",
    "                input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
    "            else:\n",
    "                input = top1\n",
    "\n",
    "            # If at inference and reach <eos> token, break early\n",
    "            if mode != 'train' and (input == 2).all():  # Assuming 2 is the <eos> token\n",
    "                break\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def train_model(self, dataloader, optimizer, criterion, clip, vocab):\n",
    "        self.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for i, (src, trg) in enumerate(dataloader):\n",
    "            src = torch.nn.utils.rnn.pad_sequence(src, padding_value=vocab[\"<pad>\"], batch_first=False).to(self.device)\n",
    "            trg = torch.nn.utils.rnn.pad_sequence(trg, padding_value=vocab[\"<pad>\"], batch_first=False).to(self.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = self(src, trg)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(self.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        return epoch_loss / len(dataloader)\n",
    "\n",
    "    def sample(self, sentence):\n",
    "        self.eval()\n",
    "        tokenizer = self.tokenizer\n",
    "        tokens = tokenizer(sentence)\n",
    "        token_indices = [self.vocab[token] for token in tokens]\n",
    "        src_tensor = torch.tensor(token_indices).unsqueeze(1).to(self.device)  # Shape: [src_len, 1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(src_tensor, mode='eval')\n",
    "        \n",
    "        output_indices = outputs.argmax(-1).squeeze().tolist()\n",
    "        output_tokens = [self.vocab.get_itos()[index] for index in output_indices if index != self.vocab[\"<pad>\"]]\n",
    "\n",
    "        # Remove everything after the <eos> token\n",
    "        if '<eos>' in output_tokens:\n",
    "            output_tokens = output_tokens[:output_tokens.index('<eos>')]\n",
    "\n",
    "        return ' '.join(output_tokens)\n",
    "\n",
    "# Example usage\n",
    "# Assuming model, vocab, and device are already defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just in case', 'a sorry sight', 'rule of thumb', 'carpe diem', 'salad days', 'off the record', 'thank goodness', 'big bucks', 'dog days', 'wet behind the ears', 'just deserts', 'an arm and a leg', 'never mind', 'bricks and mortar', 'close call', 'a sight for sore eyes', 'open warfare', 'pin money', 'third time lucky', 'race against time', 'rain or shine', 'hold on a second', 'next to nothing', 'cheek by jowl', 'black and blue', 'dead wood', 'stranger things have happened', 'pigs might fly', 'poetry in motion', 'dressed up to the nines', 'way around', 'no strings attached', 'a shot in the arm', 'below the salt', 'heart of gold', 'in the pipeline', 'golden age', 'in limbo', 'baptism of fire', 'beyond words', 'on the table', 'in a bad shape', 'on the horns of a dilemma', 'under pressure', 'dead right', 'give me five', 'in the heat of the moment', 'fit as a fiddle', 'charity begins at home', 'syrup of figs', '']\n",
      "['As a precaution', 'Something pitiful or disappointing to see', 'A general guideline', 'Seize the day', 'Youthful times', 'Unofficially', 'Luckily', 'A lot of money', 'The hottest days of summer', 'Inexperienced', 'Deserved outcome', 'Very expensive', 'Forget it', 'Physical buildings', 'Narrow escape', 'A welcome sight', 'Obvious conflict', 'Small amount of money', 'Success on the third attempt', 'Rushing to meet a deadline', 'No matter the weather', 'Wait a moment', 'Almost free', 'Very close together', 'Bruised', 'Useless people or things', 'Unlikely, but possible', 'Very unlikely', 'Graceful movement', 'Very dressed up', 'Alternative method', 'Without conditions', 'A boost', 'Of low status', 'Very kind and generous', 'Being planned or developed', 'A period of great achievement', 'In a state of uncertainty', 'A difficult introduction', 'Indescribable', 'Available for discussion', 'In poor condition', 'Facing a difficult choice', 'Stressed', 'Absolutely correct', 'High five', 'Overwhelmed by emotion', 'Very healthy', 'Take care of your family first', 'Laxative ']\n",
      "50 50\n"
     ]
    }
   ],
   "source": [
    "with open('50_idioms.txt') as f:\n",
    "    idioms = f.read()\n",
    "    idiomatic_sentences = idioms.split(\"\\n\")\n",
    "    \n",
    "with open('50_translated_idiom.txt') as f:\n",
    "    translated = f.read()\n",
    "    plain_sentences = translated.split(\"\\n\")\n",
    "    \n",
    "print(idiomatic_sentences)\n",
    "print(plain_sentences)\n",
    "idiomatic_sentences = idiomatic_sentences[0:-1]\n",
    "print(len(idiomatic_sentences), len(plain_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/41 [00:00<00:11,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 4.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 11/41 [00:03<00:08,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train Loss: 0.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 21/41 [00:05<00:05,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train Loss: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 31/41 [00:08<00:02,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Train Loss: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:11<00:00,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Train Loss: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Helper function to tokenize and build vocabulary\n",
    "def yield_tokens(data_iter, tokenizer):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "# Example idiomatic and plain sentence pairs\n",
    "# idiomatic_sentences = [\n",
    "#     \"Break a leg\",\n",
    "#     \"Once in a blue moon\",\n",
    "#     \"Spill the beans\"\n",
    "# ]\n",
    "# plain_sentences = [\n",
    "#     \"Good luck\",\n",
    "#     \"Very rarely\",\n",
    "#     \"Reveal a secret\"\n",
    "# ]\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(idiomatic_sentences + plain_sentences, tokenizer), specials=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Convert sentences to tensors\n",
    "def sentence_to_tensor(sentence, vocab, tokenizer):\n",
    "    tokens = tokenizer(sentence)\n",
    "    indexes = [vocab[token] for token in tokens]\n",
    "    return torch.tensor([vocab[\"<sos>\"]] + indexes + [vocab[\"<eos>\"]])\n",
    "\n",
    "# Parameters\n",
    "INPUT_DIM = len(vocab)\n",
    "OUTPUT_DIM = len(vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "BATCH_SIZE = 2\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "attn = Attention(HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device, vocab, tokenizer).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
    "\n",
    "# Create DataLoader\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, src_sentences, trg_sentences, vocab, tokenizer):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.trg_sentences = trg_sentences\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_tensor = sentence_to_tensor(self.src_sentences[idx], self.vocab, self.tokenizer)\n",
    "        trg_tensor = sentence_to_tensor(self.trg_sentences[idx], self.vocab, self.tokenizer)\n",
    "        return src_tensor, trg_tensor\n",
    "\n",
    "dataset = TranslationDataset(idiomatic_sentences, plain_sentences, vocab, tokenizer)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, (src, trg) in enumerate(dataloader):\n",
    "        src = torch.nn.utils.rnn.pad_sequence(src, padding_value=vocab[\"<pad>\"], batch_first=False).to(device)\n",
    "        trg = torch.nn.utils.rnn.pad_sequence(trg, padding_value=vocab[\"<pad>\"], batch_first=False).to(device)\n",
    "        # print('Source Sentence:')\n",
    "        # for idx in src.transpose(0, 1):\n",
    "        #     print(' '.join([vocab.get_itos()[i.item()] for i in idx if i != vocab[\"<pad>\"]]))\n",
    "\n",
    "        # # Print target sentence\n",
    "        # print('Target Sentence:')\n",
    "        # for idx in trg.transpose(0, 1):\n",
    "        #     print(' '.join([vocab.get_itos()[i.item()] for i in idx if i != vocab[\"<pad>\"]]))\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        # for b in range(output.size(1)):  # Iterate over batch dimension\n",
    "        #     pred_indices = output[:, b, :].argmax(dim=-1)\n",
    "        #     pred_sentence = ' '.join([vocab.get_itos()[idx.item()] for idx in pred_indices])\n",
    "        #     print(\"Predicted sentence for batch item\", b, \":\", pred_sentence)\n",
    "\n",
    "\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Convert predicted indices to tokens\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "# Start training\n",
    "for epoch in tqdm(range(41)):\n",
    "    # print(\"start of epoch\", epoch, \"========================================\")\n",
    "    train_loss = train(model, dataloader, optimizer, criterion, CLIP)\n",
    "    if epoch %10 == 0:\n",
    "        print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}')\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence: <unk> Laxative\n"
     ]
    }
   ],
   "source": [
    "sentence = idiomatic_sentences[49]\n",
    "generated_sentence = model.sample(sentence)\n",
    "print(\"Generated Sentence:\", generated_sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
